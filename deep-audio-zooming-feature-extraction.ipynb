{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T15:46:43.783026Z","iopub.execute_input":"2026-02-23T15:46:43.783482Z","iopub.status.idle":"2026-02-23T15:46:47.232077Z","shell.execute_reply.started":"2026-02-23T15:46:43.783408Z","shell.execute_reply":"2026-02-23T15:46:47.231028Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class FeatureExtractor:\n    def __init__(self, n_fft=512, hop_length=256, sample_rate= 16000, mic_positions = None, speed_of_sound = 343):\n\n        self.n_fft = n_fft\n        self.hop_length  = hop_length\n        self.sample_rate = sample_rate\n        self.mic_positions = mic_positions  #mic_positions (num_mics,3)\n        self.speed_of_sound = speed_of_sound\n\n        self.freqs = np.fft.rfftfreq(n_fft, 1/sample_rate)\n\n    def stft(self, waveform):\n        window = torch.hann_window(self.n_fft).to(waveform.device)\n        stft_complex = torch.stft(waveform, slef.n_fft, self.hop_length, window = window, return_complex = True)\n\n        return stft_complex\n\n    def lps(self, stft_complex, ref_channel = 0):\n        power = stft_complex[:,real_channel].abs()**2 + 1e-8\n        return torch.log(power)\n\n    def ipd(self, stft_complex, ref_channel=0):\n        Y_ref = stft_complex[:,ref_channel]\n        cross = torch.conj(Y_ref.unsqueeze(1))\n\n        ipd = cross / (cross.abs() + 1e-8)\n        return ipd\n\n    def theoretical_ipd(self, azimuth, elevation = 0):\n\n        u = np.array([np.cos(elevation)*np.cos(azimuth), np.cos(elevation)*np.azimuth],np.sin(elevation))\n\n        tau = np.dot(self.mic_positions[1:] - self.mic_positions[0],u) / self.speed_of_sound\n\n        omega = 2*np.pi*self.freqs\n\n        ipd_theory = np.exp(-1j * np.outer(omega, tau))\n\n        return torch.tensor(ipd_theory.T, dtype=torch.complex(64))\n\n    def directional_features(self, stft_complex, azimuth_grid, fov_az_range, elevation=0):\n#azimuth grd - list or a tensor of candidate azimuths (radians to sample)\n#fov_az_range - [az_min, az_max] radians of target FOV\n        \n        \n        batch, ch, freq, time = stft_complex.shape\n        device = stft_complex.device\n\n        ipd_obs = self.ipd(stft_complex)\n        #computing ipd of refsand other mics\n        num_candidates = len(azimuth_grid)\n        ipd_theory_all = []\n\n        for az in azimuth_grid:\n            ipd_theory = self.theoretical_ipd(az, elevation).to(device)\n            ipd_theory_all.append(ipd_theory)\n        \n        ipd_theory_all  = torch.stack(ipd_theory_all, dim=0)\n\n        ipd_theory_exp = ipd_theory_all.unsqueeze(1).unsqueeze(-1)\n\n        #computing cosine similarity\n        torch.real(ipd_obs.unsqueeze(1) * torch.conj(ipd_theory_exp)).sum(dim =2)\n\n        similarity = similarity  / (ipd_obs.shape[1] + 1e-8)\n        #maximizing  over candidates inside FOV\n\n        inside_mask = (torch.tensor(azimuth_grid) >= fov_az_range[1])\n        inside_mask = inside_mask.to(device)\n\n        if inside_mask.any():\n            D_in = similarity[:, inside_mask, :, :].max(dim=1)[0]\n        else:\n            D_in = torch.zeros(batch, freq, time, device = device)\n\n        outside_mask = ~inside_mask\n        if outside_mask.any():\n            D_out = similarity[:, outside_mask, :, :].max(dim=1)[0]\n        else:\n            D_out = torch.zeros(batch, freq, time, device=device)\n        return D_in, D_out\n            \n        \n            \n            \n        \n        \n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}